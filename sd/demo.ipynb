{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T04:46:08.288382Z",
     "start_time": "2024-02-01T04:46:04.146149Z"
    },
    "cellView": "form",
    "id": "x_dhQfFYXoPu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h8/w67nqb354zs8z6vhf7rby50m0000gn/T/ipykernel_1071/1562615225.py:16: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  elif (torch.has_mps or torch.backends.mps.is_available()) and ALLOW_MPS:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "slow_conv2d_forward_mps: input(device='cpu') and weight(device=mps:0')  must be on the same device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m num_inference_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\u001b[38;5;66;03m#50\u001b[39;00m\n\u001b[1;32m     47\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\u001b[38;5;66;03m#42\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m output_image \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     50\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     51\u001b[0m     uncond_prompt\u001b[38;5;241m=\u001b[39muncond_prompt,\n\u001b[1;32m     52\u001b[0m     input_image\u001b[38;5;241m=\u001b[39minput_image,\n\u001b[1;32m     53\u001b[0m     strength\u001b[38;5;241m=\u001b[39mstrength,\n\u001b[1;32m     54\u001b[0m     do_cfg\u001b[38;5;241m=\u001b[39mdo_cfg,\n\u001b[1;32m     55\u001b[0m     cfg_scale\u001b[38;5;241m=\u001b[39mcfg_scale,\n\u001b[1;32m     56\u001b[0m     sampler_name\u001b[38;5;241m=\u001b[39msampler,\n\u001b[1;32m     57\u001b[0m     n_inference_steps\u001b[38;5;241m=\u001b[39mnum_inference_steps,\n\u001b[1;32m     58\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m     59\u001b[0m     models\u001b[38;5;241m=\u001b[39mmodels,\n\u001b[1;32m     60\u001b[0m     device\u001b[38;5;241m=\u001b[39mDEVICE,\n\u001b[1;32m     61\u001b[0m     idle_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;66;03m#\"cpu\",\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Combine the input image and the output image into a single image.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m Image\u001b[38;5;241m.\u001b[39mfromarray(output_image)\n",
      "File \u001b[0;32m~/Desktop/circuit_design/CircuitGeneration/sd/pipeline.py:104\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompt, uncond_prompt, input_image, strength, do_cfg, cfg_scale, sampler_name, n_inference_steps, models, seed, device, idle_device, tokenizer)\u001b[0m\n\u001b[1;32m    102\u001b[0m encoder_noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(latents_shape, generator\u001b[38;5;241m=\u001b[39mgenerator, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# (Batch_Size, 4, Latents_Height, Latents_Width)\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m latents \u001b[38;5;241m=\u001b[39m encoder(input_image_tensor, encoder_noise)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Add noise to the latents (the encoded input image)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# (Batch_Size, 4, Latents_Height, Latents_Width)\u001b[39;00m\n\u001b[1;32m    108\u001b[0m sampler\u001b[38;5;241m.\u001b[39mset_strength(strength\u001b[38;5;241m=\u001b[39mstrength)\n",
      "File \u001b[0;32m~/miniconda3/envs/StableDiffusion/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/StableDiffusion/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/circuit_design/CircuitGeneration/sd/encoder.py:84\u001b[0m, in \u001b[0;36mVAE_Encoder.forward\u001b[0;34m(self, x, noise)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m):  \u001b[38;5;66;03m# Padding at downsampling should be asymmetric (see #8)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;66;03m# Pad: (Padding_Left, Padding_Right, Padding_Top, Padding_Bottom).\u001b[39;00m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;66;03m# Pad with zeros on the right and bottom.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;66;03m# (Batch_Size, Channel, Height, Width) -> (Batch_Size, Channel, Height + Padding_Top + Padding_Bottom, Width + Padding_Left + Padding_Right) = (Batch_Size, Channel, Height + 1, Width + 1)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(x, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 84\u001b[0m     x \u001b[38;5;241m=\u001b[39m module(x)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# (Batch_Size, 8, Height / 8, Width / 8) -> two tensors of shape (Batch_Size, 4, Height / 8, Width / 8)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m mean, log_variance \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mchunk(x, \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/StableDiffusion/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/StableDiffusion/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/StableDiffusion/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/StableDiffusion/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: slow_conv2d_forward_mps: input(device='cpu') and weight(device=mps:0')  must be on the same device"
     ]
    }
   ],
   "source": [
    "import model_loader\n",
    "import pipeline\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from transformers import CLIPTokenizer\n",
    "import torch\n",
    "\n",
    "# DEVICE = \"cpu\"\n",
    "DEVICE = \"mps\"\n",
    "\n",
    "ALLOW_CUDA = False\n",
    "ALLOW_MPS = False\n",
    "\n",
    "if torch.cuda.is_available() and ALLOW_CUDA:\n",
    "    DEVICE = \"cuda\"\n",
    "elif (torch.has_mps or torch.backends.mps.is_available()) and ALLOW_MPS:\n",
    "    DEVICE = \"mps\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "tokenizer = CLIPTokenizer(\"../data/vocab.json\", merges_file=\"../data/merges.txt\")\n",
    "model_file = \"../data/v1-5-pruned-emaonly.ckpt\"\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "\n",
    "## TEXT TO IMAGE\n",
    "\n",
    "# prompt = \"A dog with sunglasses, wearing comfy hat, looking at camera, highly detailed, ultra sharp, cinematic, 100mm lens, 8k resolution.\"\n",
    "# prompt = \"A cat stretching on the floor, highly detailed, ultra sharp, cinematic, 100mm lens, 8k resolution.\"\n",
    "prompt = \"8k resolution\"\n",
    "uncond_prompt = \"\"  # Also known as negative prompt\n",
    "do_cfg = True\n",
    "cfg_scale = 8  # min: 1, max: 14\n",
    "\n",
    "## IMAGE TO IMAGE\n",
    "\n",
    "# input_image = None\n",
    "# Comment to disable image to image\n",
    "image_path = \"../images/dog.jpg\"\n",
    "input_image = Image.open(image_path)\n",
    "# Higher values means more noise will be added to the input image, so the result will further from the input image.\n",
    "# Lower values means less noise is added to the input image, so output will be closer to the input image.\n",
    "strength = 0.2\n",
    "\n",
    "## SAMPLER\n",
    "\n",
    "sampler = \"ddpm\"\n",
    "num_inference_steps = 3#50\n",
    "seed = None#42\n",
    "\n",
    "output_image = pipeline.generate(\n",
    "    prompt=prompt,\n",
    "    uncond_prompt=uncond_prompt,\n",
    "    input_image=input_image,\n",
    "    strength=strength,\n",
    "    do_cfg=do_cfg,\n",
    "    cfg_scale=cfg_scale,\n",
    "    sampler_name=sampler,\n",
    "    n_inference_steps=num_inference_steps,\n",
    "    seed=seed,\n",
    "    models=models,\n",
    "    device=DEVICE,\n",
    "    idle_device=\"cpu\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Combine the input image and the output image into a single image.\n",
    "Image.fromarray(output_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(torch.utils.data.DataLoader):\n",
    "    r\"\"\"Data loader which merges data objects from a\n",
    "    :class:`torch_geometric.data.dataset` to a mini-batch.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset from which to load the data.\n",
    "        batch_size (int, optional): How many samples per batch to load.\n",
    "            (default: :obj:`1`)\n",
    "        shuffle (bool, optional): If set to :obj:`True`, the data will be\n",
    "            reshuffled at every epoch. (default: :obj:`False`)\n",
    "        follow_batch (list or tuple, optional): Creates assignment batch\n",
    "            vectors for each key in the list. (default: :obj:`[]`)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False, follow_batch=[],\n",
    "                 **kwargs):\n",
    "        def collate(batch):\n",
    "            elem = batch[0]\n",
    "            if isinstance(elem, Data):\n",
    "                return Batch.from_data_list(batch, follow_batch)\n",
    "            elif isinstance(elem, float):\n",
    "                return torch.tensor(batch, dtype=torch.float)\n",
    "            elif isinstance(elem, int_classes):\n",
    "                return torch.tensor(batch)\n",
    "            elif isinstance(elem, string_classes):\n",
    "                return batch\n",
    "            elif isinstance(elem, container_abcs.Mapping):\n",
    "                return {key: collate([d[key] for d in batch]) for key in elem}\n",
    "            elif isinstance(elem, tuple) and hasattr(elem, '_fields'):\n",
    "                return type(elem)(*(collate(s) for s in zip(*batch)))\n",
    "            elif isinstance(elem, container_abcs.Sequence):\n",
    "                return [collate(s) for s in zip(*batch)]\n",
    "\n",
    "            raise TypeError('DataLoader found invalid type: {}'.format(\n",
    "                type(elem)))\n",
    "\n",
    "        super(DataLoader,\n",
    "              self).__init__(dataset, batch_size, shuffle,\n",
    "                             collate_fn=lambda batch: collate(batch), **kwargs)\n",
    "device = 'mps'\n",
    "model.to(device).reset_parameters()\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "train_dataset,train_des, test_dataset, test_des = [],[],[],[]\n",
    "for index, d in enumerate(dataset):\n",
    "    if d.set=='train':\n",
    "        des = (torch.tensor(netlsd.heat(to_networkx(d, to_undirected = True)))*0.1).float()\n",
    "        des_d = Data(x = des, edge_index = d.edge_index, y = d.y)\n",
    "        train_dataset.append(d)\n",
    "        train_des.append(des_d)\n",
    "\n",
    "torch.save(train_des,dataset_name+'_netlsd_train.pt')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
    "train_loader_des = DataLoader(train_des, batch_size=128, shuffle=False)\n",
    "\n",
    "def train_FEGIN(model, optimizer, loader,des_loader, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data,des in zip(loader,des_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        des = des.to(device)\n",
    "        out = model(data,des)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yuxuanli/Desktop/circuit_design/pytorch-stable-diffusion-main/sd\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T20:11:10.510128Z",
     "start_time": "2024-01-30T20:11:10.382969Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yuxuanli/Desktop/circuit_design/pytorch-stable-diffusion-main/sd\r\n"
     ]
    }
   ],
   "source": [
    "def rescale(x, old_range, new_range, clamp=False):\n",
    "    old_min, old_max = old_range\n",
    "    new_min, new_max = new_range\n",
    "    x -= old_min\n",
    "    x *= (new_max - new_min) / (old_max - old_min)\n",
    "    x += new_min\n",
    "    if clamp:\n",
    "        x = x.clamp(new_min, new_max)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T20:31:25.515446Z",
     "start_time": "2024-01-30T20:31:25.477020Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "device='mps'\n",
    "WIDTH = 512\n",
    "HEIGHT = 512\n",
    "LATENTS_WIDTH = WIDTH // 8\n",
    "LATENTS_HEIGHT = HEIGHT // 8\n",
    "latents_shape = (1, 4, LATENTS_HEIGHT, LATENTS_WIDTH)\n",
    "# input_image = None\n",
    "generator = torch.Generator(device=device)\n",
    "generator.seed()\n",
    "encoder = models[\"encoder\"]\n",
    "encoder.to(device)\n",
    "            \n",
    "image_path = \"./images/dog.jpg\"\n",
    "input_image = Image.open(image_path)\n",
    "input_image_tensor = input_image.resize((WIDTH, HEIGHT))\n",
    "input_image_tensor = np.array(input_image_tensor)\n",
    "input_image_tensor = torch.tensor(input_image_tensor, dtype=torch.float32)\n",
    "input_image_tensor = rescale(input_image_tensor, (0, 255), (-1, 1))\n",
    "input_image_tensor = input_image_tensor.unsqueeze(0)\n",
    "input_image_tensor = input_image_tensor.permute(0, 3, 1, 2)\n",
    "encoder_noise = torch.randn(latents_shape, generator=generator, device=device)\n",
    "print('input_image_tensor', input_image_tensor.size())\n",
    "print('encoder_noise', encoder_noise.size())\n",
    "latents = encoder(input_image_tensor, encoder_noise)\n",
    "\n",
    "print(input_image_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yuxuanli/Desktop/circuit_design/pytorch-stable-diffusion-main/sd\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T01:58:43.450836Z",
     "start_time": "2024-02-01T01:58:21.493141Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image-before (1212, 1106)\n",
      "Image-after-rescale (512, 512)\n",
      "input_image_tensor-4 786432\n",
      "******************** 512\n",
      "******************** 512\n",
      "******************** 3\n",
      "******************** 172\n",
      "input_image_tensor-3 torch.Size([512, 512, 3])\n",
      "input_image_tensor-2 torch.Size([512, 512, 3])\n",
      "input_image_tensor-1 torch.Size([1, 512, 512, 3])\n",
      "input_image_tensor torch.Size([1, 3, 512, 512])\n",
      "encoder_noise torch.Size([1, 4, 64, 64])\n",
      "torch.Size([1, 4, 64, 64]) torch.Size([1, 4, 64, 64]) torch.Size([1, 4, 64, 64])\n",
      "latents-end torch.Size([1, 4, 64, 64])\n",
      "torch.Size([1, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "latents_shape = (1, 3, LATENTS_HEIGHT, LATENTS_WIDTH)\n",
    "input_image_tensor = torch.randn((1, 3, 512, 512), generator=generator, device=device)\n",
    "\n",
    "encoder_noise = torch.randn(latents_shape, generator=generator, device=device)\n",
    "print('input_image_tensor', input_image_tensor.size())\n",
    "print('encoder_noise', encoder_noise.size())\n",
    "latents = encoder(input_image_tensor, encoder_noise)\n",
    "\n",
    "print(input_image_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from decoder import VAE_AttentionBlock, VAE_ResidualBlock\n",
    "\n",
    "class VAE_Encoder(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            # (Batch_Size, Channel, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            \n",
    "             # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
    "            VAE_ResidualBlock(128, 128),\n",
    "            \n",
    "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
    "            VAE_ResidualBlock(128, 128),\n",
    "            \n",
    "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height / 2, Width / 2)\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=0),\n",
    "            \n",
    "            # (Batch_Size, 128, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n",
    "            VAE_ResidualBlock(128, 256), \n",
    "            \n",
    "            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n",
    "            VAE_ResidualBlock(256, 256), \n",
    "            \n",
    "            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 4, Width / 4)\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=0), \n",
    "            \n",
    "            # (Batch_Size, 256, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n",
    "            VAE_ResidualBlock(256, 512), \n",
    "            \n",
    "            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n",
    "            VAE_ResidualBlock(512, 512), \n",
    "            \n",
    "            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=0), \n",
    "            \n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            VAE_ResidualBlock(512, 512), \n",
    "            \n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            VAE_ResidualBlock(512, 512), \n",
    "            \n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            VAE_ResidualBlock(512, 512), \n",
    "            \n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            VAE_AttentionBlock(512), \n",
    "            \n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            VAE_ResidualBlock(512, 512), \n",
    "            \n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            nn.GroupNorm(32, 512), \n",
    "            \n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            nn.SiLU(), \n",
    "\n",
    "            # Because the padding=1, it means the width and height will increase by 2\n",
    "            # Out_Height = In_Height + Padding_Top + Padding_Bottom\n",
    "            # Out_Width = In_Width + Padding_Left + Padding_Right\n",
    "            # Since padding = 1 means Padding_Top = Padding_Bottom = Padding_Left = Padding_Right = 1,\n",
    "            # Since the Out_Width = In_Width + 2 (same for Out_Height), it will compensate for the Kernel size of 3\n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 8, Height / 8, Width / 8). \n",
    "            nn.Conv2d(512, 8, kernel_size=3, padding=1), \n",
    "\n",
    "            # (Batch_Size, 8, Height / 8, Width / 8) -> (Batch_Size, 8, Height / 8, Width / 8)\n",
    "            nn.Conv2d(8, 8, kernel_size=1, padding=0), \n",
    "        )\n",
    "\n",
    "    def forward(self, x, noise):\n",
    "        # x: (Batch_Size, Channel, Height, Width)\n",
    "        # noise: (Batch_Size, 4, Height / 8, Width / 8)\n",
    "\n",
    "        for module in self:\n",
    "\n",
    "            if getattr(module, 'stride', None) == (2, 2):  # Padding at downsampling should be asymmetric (see #8)\n",
    "                # Pad: (Padding_Left, Padding_Right, Padding_Top, Padding_Bottom).\n",
    "                # Pad with zeros on the right and bottom.\n",
    "                # (Batch_Size, Channel, Height, Width) -> (Batch_Size, Channel, Height + Padding_Top + Padding_Bottom, Width + Padding_Left + Padding_Right) = (Batch_Size, Channel, Height + 1, Width + 1)\n",
    "                x = F.pad(x, (0, 1, 0, 1))\n",
    "            \n",
    "            x = module(x)\n",
    "        # (Batch_Size, 8, Height / 8, Width / 8) -> two tensors of shape (Batch_Size, 4, Height / 8, Width / 8)\n",
    "        print(x.size())\n",
    "        mean, log_variance = torch.chunk(x, 2, dim=1)\n",
    "        # Clamp the log variance between -30 and 20, so that the variance is between (circa) 1e-14 and 1e8. \n",
    "        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
    "        log_variance = torch.clamp(log_variance, -30, 20)\n",
    "        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
    "        variance = log_variance.exp()\n",
    "        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
    "        stdev = variance.sqrt()\n",
    "        \n",
    "        # Transform N(0, 1) -> N(mean, stdev) \n",
    "        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
    "        print('*********************')\n",
    "        # print(mean.size(),stdev.size(),noise.size())\n",
    "        # print(mean.size())\n",
    "        # print(stdev.size())\n",
    "        # print(noise.size())\n",
    "        x = mean + stdev * noise\n",
    "        \n",
    "        # Scale by a constant\n",
    "        # Constant taken from: https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/configs/stable-diffusion/v1-inference.yaml#L17C1-L17C1\n",
    "        x *= 0.18215\n",
    "        \n",
    "        return x\n",
    "import os\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "import model_converter\n",
    "ckpt_path = \"./data/v1-5-pruned-emaonly.ckpt\"\n",
    "device = 'mps'\n",
    "# from encoder import VAE_Encoder\n",
    "state_dict = model_converter.load_from_standard_weights(ckpt_path, device)\n",
    "\n",
    "encoder = VAE_Encoder().to(device)\n",
    "encoder.load_state_dict(state_dict['encoder'], strict=True)\n",
    "# latents_shape = (1, 3, LATENTS_HEIGHT, LATENTS_WIDTH)\n",
    "input_image_tensor = torch.randn((1, 3, 512, 512), generator=generator, device=device)\n",
    "encoder_noise = torch.randn((1, 3, 512, 512), generator=generator, device=device)\n",
    "\n",
    "encoder(input_image_tensor, encoder_noise)\n",
    "print(encoder.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T20:33:00.496308Z",
     "start_time": "2024-01-30T20:32:06.763420Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([1, 8, 64, 64])\n",
      "*********************\n",
      "mean torch.Size([1, 4, 64, 64])\n",
      "stdev torch.Size([1, 4, 64, 64])\n",
      "noise torch.Size([1, 4, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.4864, -0.9815, -0.9484,  ..., -0.0782, -0.3435,  0.3140],\n",
       "          [-0.2818, -0.8840, -1.8635,  ..., -1.5010, -1.5107,  0.1602],\n",
       "          [-1.7077, -0.2086, -0.1513,  ..., -1.7995, -0.0250, -0.4913],\n",
       "          ...,\n",
       "          [-0.2084, -1.4657, -0.7528,  ..., -1.5601, -1.0433, -1.7112],\n",
       "          [-1.7342, -0.0391, -1.1097,  ...,  0.2330,  0.0352,  0.4810],\n",
       "          [ 0.4188, -0.8334, -1.1448,  ..., -1.1172, -1.7300,  0.4964]],\n",
       "\n",
       "         [[ 0.7334,  0.8133,  1.0648,  ...,  0.7571,  1.2425, -0.0194],\n",
       "          [ 0.7928,  0.8182,  0.1353,  ...,  1.2498,  0.4588,  0.6315],\n",
       "          [ 0.2804,  0.6980,  1.0720,  ...,  0.6719,  0.6107,  1.1649],\n",
       "          ...,\n",
       "          [ 0.9424,  0.9101,  1.2504,  ...,  0.1707,  0.3486,  1.0858],\n",
       "          [ 0.1542,  0.7544,  0.8948,  ...,  0.8603,  0.9035,  0.4953],\n",
       "          [ 0.8320,  0.5112,  0.4308,  ...,  0.2906,  0.4974,  0.3976]],\n",
       "\n",
       "         [[ 0.1387, -0.8766, -1.4363,  ..., -0.1257, -1.7065, -0.5799],\n",
       "          [ 0.2318,  0.0529, -0.3802,  ..., -0.3721,  0.3529,  0.8411],\n",
       "          [-0.8097, -0.6219, -0.3060,  ..., -1.0287, -1.0412, -0.5813],\n",
       "          ...,\n",
       "          [ 0.3143,  1.1596, -0.1768,  ..., -0.0468, -0.0278,  0.0119],\n",
       "          [-0.6711, -0.9648, -0.8976,  ...,  0.2463, -0.6983, -0.3115],\n",
       "          [ 0.9399,  0.0397, -0.6996,  ..., -1.1493, -0.1393,  0.7197]],\n",
       "\n",
       "         [[-1.0330, -1.1731, -0.4335,  ...,  0.1805, -0.2633,  0.1756],\n",
       "          [ 0.1258, -0.1497, -1.6414,  ..., -0.9236, -0.5461,  0.7402],\n",
       "          [-1.2881, -0.0178,  0.1394,  ..., -1.2880, -0.4210,  0.2675],\n",
       "          ...,\n",
       "          [-0.0348, -0.4970, -0.5522,  ..., -1.5359, -0.8373, -0.5540],\n",
       "          [-1.3319, -0.7250, -0.2909,  ...,  0.3961,  0.0750,  0.5872],\n",
       "          [ 0.7266, -0.3823, -0.9394,  ..., -1.1600, -1.1479,  0.8934]]]],\n",
       "       device='mps:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yuxuanli/Desktop/circuit_design/pytorch-stable-diffusion-main/sd\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "iDI2dKfRWTId"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
